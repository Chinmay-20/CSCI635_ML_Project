# -*- coding: utf-8 -*-
"""BERT_model_only

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ut2sdLbx1JVXt2g4OY0ksyg2rSj8YoaU
"""

import pandas as pd
import numpy as np
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from scipy.special import softmax

df = pd.read_csv('Reviews.csv', encoding='ISO-8859-1')

# Select and rename relevant columns
df = df[['HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Summary', 'Text']]
df = df.rename(columns={'Score': 'Rating', 'Text': 'Review'})

# Drop duplicate reviews
df.drop_duplicates('Review', keep='first', inplace=True)

# Remove rows with HelpfulnessNumerator > HelpfulnessDenominator
df = df[df['HelpfulnessNumerator'] <= df['HelpfulnessDenominator']]

# Reset index and select columns for sentiment analysis
df = df.reset_index(drop=True)
df = df[['Rating', 'Summary', 'Review']]

# Map ratings to sentiment labels
convert_dict = {5: 'Positive', 4: 'Positive', 3: 'Neutral', 2: 'Negative', 1: 'Negative'}
df['Sentiment'] = df['Rating'].map(convert_dict)

# Visualize word clouds for each sentiment category
def plot_wordcloud(text, title):
    wordcloud = WordCloud(width=800, height=400, stopwords=STOPWORDS, background_color="black").generate(text)
    plt.figure(figsize=(10, 7))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis('off')
    plt.title(title)
    plt.show()

# Generate word clouds for Positive, Neutral, and Negative sentiments
for sentiment in df['Sentiment'].unique():
    sentiment_text = ' '.join(df[df['Sentiment'] == sentiment]['Review'].dropna().astype(str))
    plot_wordcloud(sentiment_text, f"Word Cloud for {sentiment} Sentiments")

# Load BERT tokenizer and model for sequence classification
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=3)

# Preprocess text data for BERT
def preprocess_for_bert(reviews):
    return tokenizer(reviews, padding=True, truncation=True, max_length=128, return_tensors="pt")

# Predict sentiment using BERT
def predict_sentiment(review):
    inputs = preprocess_for_bert([review])
    with torch.no_grad():
        logits = model(**inputs).logits
    probabilities = softmax(logits.numpy(), axis=1)
    sentiment = np.argmax(probabilities)
    sentiment_labels = {0: "Negative", 1: "Neutral", 2: "Positive"}
    return sentiment_labels[sentiment], probabilities

sample_review = df['Review'][0]
predicted_sentiment, probabilities = predict_sentiment(sample_review)
print(f"Review: {sample_review}")
print(f"Predicted Sentiment: {predicted_sentiment}, Probabilities: {probabilities}")

# Define a directory to save the model and tokenizer
save_directory = "./saved_bert_model"

# Save the model
model.save_pretrained(save_directory)

# Save the tokenizer
tokenizer.save_pretrained(save_directory)
